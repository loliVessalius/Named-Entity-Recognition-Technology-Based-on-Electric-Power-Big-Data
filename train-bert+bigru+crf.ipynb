{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1、导包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-16T14:31:18.506765Z",
     "iopub.status.busy": "2023-04-16T14:31:18.506154Z",
     "iopub.status.idle": "2023-04-16T14:31:18.513343Z",
     "shell.execute_reply": "2023-04-16T14:31:18.512418Z",
     "shell.execute_reply.started": "2023-04-16T14:31:18.506725Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from functools import partial\n",
    "# 导入paddle库\n",
    "import paddle\n",
    "import paddle.nn as nn\n",
    "from paddlenlp.transformers import  BertModel\n",
    "from paddle.io import DataLoader\n",
    "# 导入paddlenlp的库\n",
    "from paddlenlp.transformers import LinearDecayWithWarmup\n",
    "from paddlenlp.metrics import ChunkEvaluator\n",
    "from paddlenlp.transformers import BertTokenizer,BertPretrainedModel\n",
    "from paddlenlp.data import Stack, Tuple, Pad, Dict\n",
    "\n",
    "# 读取数据\n",
    "from paddlenlp.datasets import load_dataset\n",
    "from paddlenlp.layers.crf import LinearChainCrf, ViterbiDecoder, LinearChainCrfLoss\n",
    "\n",
    "# 导入自定义的工具类\n",
    "from utils import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-16T14:31:18.515466Z",
     "iopub.status.busy": "2023-04-16T14:31:18.515079Z",
     "iopub.status.idle": "2023-04-16T14:31:18.536315Z",
     "shell.execute_reply": "2023-04-16T14:31:18.535252Z",
     "shell.execute_reply.started": "2023-04-16T14:31:18.515432Z"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 从dev.txt和train.txt文件生成dic\n",
    "gernate_dic('data1/dev.txt', 'data1/train.txt', 'data1/tag.dic')\n",
    "id2label,label2id,label_list = load_dicts('data1/tag.dic')\n",
    "print(id2label)\n",
    "print(label2id)\n",
    "print(label_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2、把文本和label映射成id，处理成模型的输入的形式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-16T14:31:18.537931Z",
     "iopub.status.busy": "2023-04-16T14:31:18.537576Z",
     "iopub.status.idle": "2023-04-16T14:31:18.556127Z",
     "shell.execute_reply": "2023-04-16T14:31:18.555046Z",
     "shell.execute_reply.started": "2023-04-16T14:31:18.537905Z"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "label_map=label2id\n",
    "def read(data_path):\n",
    "    with open(data_path, 'r', encoding='utf-8') as f:\n",
    "        # 跳过列名\n",
    "        for line in f.readlines():\n",
    "            words, labels = line.strip('\\n').split('\\t')\n",
    "            words = words.split()\n",
    "            labels = labels.split()\n",
    "            labels=[label_map[item] for item in labels]\n",
    "            yield {'tokens': words, 'labels': labels}\n",
    "\n",
    "# data_path为read()方法的参数\n",
    "train_ds = load_dataset(read, data_path='data1/train.txt',lazy=False)\n",
    "dev_ds = load_dataset(read, data_path='data1/dev.txt',lazy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-16T14:31:18.557602Z",
     "iopub.status.busy": "2023-04-16T14:31:18.557315Z",
     "iopub.status.idle": "2023-04-16T14:31:18.563755Z",
     "shell.execute_reply": "2023-04-16T14:31:18.562863Z",
     "shell.execute_reply.started": "2023-04-16T14:31:18.557576Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def convert_example_to_feature(example, tokenizer, no_entity_id,\n",
    "                              max_seq_len=512):\n",
    "    labels = example['labels']\n",
    "    example = example['tokens']\n",
    "    tokenized_input = tokenizer(\n",
    "        example,\n",
    "        return_length=True,\n",
    "        is_split_into_words=True,\n",
    "        max_seq_len=max_seq_len)\n",
    "\n",
    "    # -2 for [CLS] and [SEP]\n",
    "    if len(tokenized_input['input_ids']) - 2 < len(labels):\n",
    "        labels = labels[:len(tokenized_input['input_ids']) - 2]\n",
    "    tokenized_input['labels'] = [no_entity_id] + labels + [no_entity_id]\n",
    "    tokenized_input['labels'] += [no_entity_id] * (\n",
    "        len(tokenized_input['input_ids']) - len(tokenized_input['labels']))\n",
    "    return tokenized_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "利用DataLoader将把处理好的数据输送给模型，下面是训练集和测试集的Dataloader构建过程。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 3、加载数据集train_data_loader和dev_data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-16T14:31:18.566430Z",
     "iopub.status.busy": "2023-04-16T14:31:18.565880Z",
     "iopub.status.idle": "2023-04-16T14:31:18.594560Z",
     "shell.execute_reply": "2023-04-16T14:31:18.593526Z",
     "shell.execute_reply.started": "2023-04-16T14:31:18.566380Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_seq_length=128\n",
    "batch_size=16\n",
    "label_num = len(label_list)\n",
    "no_entity_id = label_num - 1\n",
    "\n",
    "#以下4个参数在在bilstm用到\n",
    "gru_hidden_size, num_layers, num_labels, dropout = 256,2,len(list(id2label)),0.1\n",
    "\n",
    "model_name_or_path='bert-base-chinese'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name_or_path)\n",
    "\n",
    "trans_func = partial(\n",
    "        convert_example_to_feature,\n",
    "        tokenizer=tokenizer,\n",
    "        no_entity_id=no_entity_id,\n",
    "        max_seq_len=max_seq_length)\n",
    "        \n",
    "train_ds = train_ds.map(trans_func)\n",
    "dev_ds = dev_ds.map(trans_func)\n",
    "\n",
    "ignore_label = label_num - 1\n",
    "\n",
    "batchify_fn = lambda samples, fn=Dict({\n",
    "        'input_ids': Pad(axis=0, pad_val=tokenizer.pad_token_id, dtype='int32'),  # input\n",
    "        'token_type_ids': Pad(axis=0, pad_val=tokenizer.pad_token_type_id, dtype='int32'),  # segment\n",
    "        'seq_len': Stack(dtype='int64'),  # seq_len\n",
    "        'labels': Pad(axis=0, pad_val=ignore_label, dtype='int64')  # label\n",
    "    }): fn(samples)\n",
    "\n",
    "\n",
    "train_batch_sampler = paddle.io.DistributedBatchSampler(train_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "train_data_loader = DataLoader(dataset=train_ds,collate_fn=batchify_fn,num_workers=0,batch_sampler=train_batch_sampler,\n",
    "        return_list=True)\n",
    "dev_data_loader = DataLoader(dataset=dev_ds,collate_fn=batchify_fn,num_workers=0,batch_size=batch_size,return_list=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 4、定义模型初始化方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-16T14:31:18.596339Z",
     "iopub.status.busy": "2023-04-16T14:31:18.595895Z",
     "iopub.status.idle": "2023-04-16T14:31:18.605549Z",
     "shell.execute_reply": "2023-04-16T14:31:18.604232Z",
     "shell.execute_reply.started": "2023-04-16T14:31:18.596310Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Bert_BiGRU_crf(BertPretrainedModel):\n",
    "\n",
    "    def __init__(self, bert,num_classes, gru_hidden_size, num_layers,dropout):\n",
    "        super(Bert_BiGRU_crf, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        # 初始化bert\n",
    "        self.bert = bert  \n",
    "        # 初始化双向的lstm\n",
    "        self.gru = nn.GRU(self.bert.config[\"hidden_size\"],gru_hidden_size,num_layers,direction='bidirect',dropout=dropout)\n",
    "        \n",
    "        self.fc = nn.Linear(gru_hidden_size * 2, self.num_classes)\n",
    "\n",
    "        # crf层\n",
    "        self.crf = LinearChainCrf(\n",
    "            self.num_classes, crf_lr=100, with_start_stop_tag=False)\n",
    "        self.crf_loss = LinearChainCrfLoss(self.crf)\n",
    "        self.viterbi_decoder = ViterbiDecoder(\n",
    "            self.crf.transitions, with_start_stop_tag=False)\n",
    "\n",
    "    def forward(self,\n",
    "                input_ids,\n",
    "                token_type_ids,\n",
    "                lengths=None,\n",
    "                labels=None):\n",
    "        sequence_out, _ = self.bert(input_ids,\n",
    "                                    token_type_ids=token_type_ids)\n",
    "        lstm_output, _ = self.gru(sequence_out)\n",
    "        emission = self.fc(lstm_output)\n",
    "  \n",
    "        if labels is not None:\n",
    "            loss = self.crf_loss(emission, lengths, labels)\n",
    "            return loss\n",
    "        else:\n",
    "            _, prediction = self.viterbi_decoder(emission, lengths)\n",
    "            return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 5、配置全局参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-16T14:31:18.607865Z",
     "iopub.status.busy": "2023-04-16T14:31:18.607326Z",
     "iopub.status.idle": "2023-04-16T14:31:18.615173Z",
     "shell.execute_reply": "2023-04-16T14:31:18.614277Z",
     "shell.execute_reply.started": "2023-04-16T14:31:18.607825Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 设置epoch\n",
    "num_train_epochs=12\n",
    "warmup_steps=0\n",
    "\n",
    "max_steps=-1\n",
    "# 优化器的超参数\n",
    "learning_rate=5e-5\n",
    "adam_epsilon=1e-8\n",
    "weight_decay=0.0\n",
    "\n",
    "global_step = 0\n",
    "# 日志输出的step数\n",
    "logging_steps=30\n",
    "\n",
    "tic_train = time.time()\n",
    "save_steps=20\n",
    "output_dir='model'\n",
    "os.makedirs(output_dir,exist_ok=True)\n",
    "\n",
    "# Define the model netword and its loss\n",
    "last_step = num_train_epochs * len(train_data_loader)\n",
    "\n",
    "num_training_steps = max_steps if max_steps > 0 else len(train_data_loader) * num_train_epochs\n",
    "lr_scheduler = LinearDecayWithWarmup(learning_rate, num_training_steps,\n",
    "                                         warmup_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 6、初始化模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-16T14:31:18.616891Z",
     "iopub.status.busy": "2023-04-16T14:31:18.616374Z",
     "iopub.status.idle": "2023-04-16T14:31:20.143576Z",
     "shell.execute_reply": "2023-04-16T14:31:20.142491Z",
     "shell.execute_reply.started": "2023-04-16T14:31:18.616867Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "bert = BertModel.from_pretrained(\"bert-base-chinese\")\n",
    "bert_bigru_crf = Bert_BiGRU_crf(bert,label_num,gru_hidden_size,num_layers,dropout)\n",
    "\n",
    "decay_params = [\n",
    "        p.name for n, p in bert_bigru_crf.named_parameters()\n",
    "        if not any(nd in n for nd in [\"bias\", \"norm\"])\n",
    "    ]\n",
    "# 设置优化器\n",
    "optimizer = paddle.optimizer.AdamW(\n",
    "        learning_rate=lr_scheduler,\n",
    "        epsilon=adam_epsilon,\n",
    "        parameters=bert_bigru_crf.parameters(),\n",
    "        weight_decay=weight_decay,\n",
    "        apply_decay_param_fun=lambda x: x in decay_params)\n",
    "# 设置损失函数\n",
    "loss_fct = nn.loss.CrossEntropyLoss(ignore_index=ignore_label)\n",
    "# 设置评估函数\n",
    "metric = ChunkEvaluator(label_list=label_list,suffix=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 7、模型评估方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-16T14:31:20.146249Z",
     "iopub.status.busy": "2023-04-16T14:31:20.145590Z",
     "iopub.status.idle": "2023-04-16T14:31:20.153921Z",
     "shell.execute_reply": "2023-04-16T14:31:20.152438Z",
     "shell.execute_reply.started": "2023-04-16T14:31:20.146201Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "@paddle.no_grad()\n",
    "def evaluate_v1(model, metric, data_loader):\n",
    "    model.eval()\n",
    "    metric.reset()\n",
    "    for input_ids, seg_ids, lens, labels in data_loader:\n",
    "        preds = model(input_ids, seg_ids, lengths=lens)\n",
    "        n_infer, n_label, n_correct = metric.compute(lens, preds, labels)\n",
    "        metric.update(n_infer.numpy(), n_label.numpy(), n_correct.numpy())\n",
    "        precision, recall, f1_score = metric.accumulate()\n",
    "    print(\"[EVAL] Precision: %f - Recall: %f - F1: %f\" % (precision, recall, f1_score))\n",
    "    model.train()\n",
    "    return precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 8、模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-16T14:31:20.155909Z",
     "iopub.status.busy": "2023-04-16T14:31:20.155479Z",
     "iopub.status.idle": "2023-04-16T14:34:01.007819Z",
     "shell.execute_reply": "2023-04-16T14:34:01.006766Z",
     "shell.execute_reply.started": "2023-04-16T14:31:20.155867Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "global_step=0\n",
    "best_pre = 0\n",
    "for epoch in range(num_train_epochs):\n",
    "    for step, batch in enumerate(train_data_loader):\n",
    "        global_step += 1\n",
    "        input_ids, token_type_ids, lengths, labels = batch\n",
    "        loss = bert_bigru_crf(input_ids, token_type_ids, lengths=lengths, labels=labels)\n",
    "        avg_loss = paddle.mean(loss)\n",
    "        if global_step % logging_steps == 0:\n",
    "            print(\"global step %d, epoch: %d, batch: %d, loss: %f, speed: %.2f step/s\"\n",
    "                    % (global_step, epoch, step, avg_loss,\n",
    "                       logging_steps / (time.time() - tic_train)))\n",
    "            tic_train = time.time()\n",
    "        avg_loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.clear_grad()\n",
    "\n",
    "    pre = evaluate_v1(bert_bigru_crf, metric, dev_data_loader)\n",
    "    # 如果pre比历史记录高，则保存新的模型\n",
    "    if pre>best_pre:\n",
    "        model_path=os.path.join(output_dir,\"bert_bigru_crf.pdparams\")\n",
    "        paddle.save(bert_bigru_crf.state_dict(),model_path)\n",
    "        tokenizer.save_pretrained(output_dir)\n",
    "        best_pre = pre"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2.1",
   "language": "python",
   "name": "tf2.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
